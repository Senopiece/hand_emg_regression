{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b11a46fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, NamedTuple\n",
    "import numpy as np\n",
    "import yaml\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "# NOTE: `frame` here refers to hand pose angles\n",
    "\n",
    "W = 64\n",
    "\n",
    "\n",
    "class HandEmgTuple(NamedTuple):\n",
    "    frame: np.ndarray  # (20,), float32 expected\n",
    "    emg: np.ndarray  # (W, C), float32 expected\n",
    "\n",
    "\n",
    "class HandEmgRecordingSegment(NamedTuple):\n",
    "    couples: List[HandEmgTuple]\n",
    "    sigma: np.ndarray  # (20,), float32 single final frame\n",
    "\n",
    "    @property\n",
    "    def emg(self):\n",
    "        return np.concatenate([s.emg for s in self.couples])\n",
    "\n",
    "    @property\n",
    "    def frames(self):\n",
    "        return np.stack([s.frame for s in self.couples] + [self.sigma])\n",
    "\n",
    "\n",
    "HandEmgRecording = List[HandEmgRecordingSegment]\n",
    "\n",
    "\n",
    "class RecordingWriter:\n",
    "    \"\"\"\n",
    "    A context for writing recording by segments\n",
    "\n",
    "    Each segment is written in format:\n",
    "    [ [<20 x float32: frame>, <W x C float32: emg>], [...], ... <20 x float32: sigma frame> ]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, context: \"DatasetWriter\", index: int):\n",
    "        self.context = context\n",
    "        self.index = index\n",
    "        self.count = 0\n",
    "\n",
    "    def add(self, segment: HandEmgRecordingSegment):\n",
    "        \"\"\"\n",
    "        Add a single recording segment to the ZIP archive.\n",
    "\n",
    "        Args:\n",
    "            segment: A list of HandEmgTuple samples. Each sample is stored with its frame\n",
    "                       (20 float32 values) and its emg (W x C float32 values). The number of EMG\n",
    "                       channels (C) is determined from the first sample and is assumed to be consistent.\n",
    "        \"\"\"\n",
    "        if self.context.archive is None:\n",
    "            raise RuntimeError(\"Archive is not open. Use 'with' statement to open it.\")\n",
    "\n",
    "        bio = io.BytesIO()\n",
    "\n",
    "        # Determine the number of EMG channels (C) from the first sample.\n",
    "        C = segment.couples[0].emg.shape[1]\n",
    "        if self.context.C is None:\n",
    "            # Store C for metadata\n",
    "            self.context.C = C\n",
    "            self.context.archive.writestr(\"metadata.yml\", yaml.dump({\"C\": C}))\n",
    "\n",
    "        elif self.context.C != C:\n",
    "            raise ValueError(\"Inconsistent number of EMG channels across recordings.\")\n",
    "\n",
    "        # Write each sample: frame (20 float32 values) then emg (W * C float32 values).\n",
    "        for tup in segment.couples:\n",
    "            # Verify data types and dimensions.\n",
    "            assert (\n",
    "                tup.frame.dtype == np.float32\n",
    "            ), f\"Frame dtype must be float32, got {tup.frame.dtype}\"\n",
    "            assert (\n",
    "                tup.emg.dtype == np.float32\n",
    "            ), f\"EMG dtype must be float32, got {tup.emg.dtype}\"\n",
    "            assert tup.frame.shape == (\n",
    "                20,\n",
    "            ), f\"Frame shape must be (20,), got {tup.frame.shape}\"\n",
    "            assert (\n",
    "                tup.emg.shape[0] == W and tup.emg.shape[1] == C\n",
    "            ), f\"EMG shape must be ({W}, {C}), got {tup.emg.shape}\"\n",
    "\n",
    "            bio.write(tup.frame.tobytes())\n",
    "            bio.write(tup.emg.flatten().tobytes())\n",
    "\n",
    "        # Write the final frame (sigma) as well.\n",
    "        bio.write(segment.sigma.tobytes())\n",
    "\n",
    "        # Save the segment\n",
    "        self.context.archive.writestr(\n",
    "            f\"recordings/{self.index}/segments/{self.count}\", bio.getvalue()\n",
    "        )\n",
    "        self.count += 1\n",
    "\n",
    "\n",
    "class DatasetWriter:\n",
    "    \"\"\"\n",
    "    A context manager for writing segments to a ZIP archive in a proprietary binary format.\n",
    "\n",
    "    Archive looks like this:\n",
    "\n",
    "    dataset.zip/\n",
    "      metadata.yml\n",
    "      recordings/\n",
    "        1/\n",
    "          segments/\n",
    "           1\n",
    "           2\n",
    "        2/\n",
    "          segments/\n",
    "            1\n",
    "            2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename: str):\n",
    "        self.filename = filename\n",
    "        self.archive = None\n",
    "        self.recording_index = 0\n",
    "        self.C: int | None = None  # To store the number of EMG channels\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.archive = zipfile.ZipFile(\n",
    "            self.filename,\n",
    "            mode=\"w\",\n",
    "            compression=zipfile.ZIP_DEFLATED,\n",
    "            compresslevel=9,\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.archive is not None:\n",
    "            self.archive.close()\n",
    "\n",
    "    def add_recording(self):\n",
    "        self.recording_index += 1\n",
    "        return RecordingWriter(self, self.recording_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14b2ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def rotation_matrix_from_vectors(\n",
    "    vec1: torch.Tensor,\n",
    "    vec2: torch.Tensor,\n",
    "    eps: float = 1e-6,\n",
    ") -> torch.Tensor:\n",
    "    a = vec1 / vec1.norm()\n",
    "    b = vec2 / vec2.norm()\n",
    "    v = torch.cross(a, b, dim=-1)\n",
    "    if v.norm() < eps:\n",
    "        return torch.eye(3, dtype=vec1.dtype, device=vec1.device)\n",
    "    c = torch.dot(a, b)\n",
    "    s = v.norm()\n",
    "    K = torch.tensor(\n",
    "        [[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]],\n",
    "        dtype=vec1.dtype,\n",
    "        device=vec1.device,\n",
    "    )\n",
    "    R = torch.eye(3, dtype=vec1.dtype, device=vec1.device)\n",
    "    return R + K + K @ K * ((1 - c) / (s * s))\n",
    "\n",
    "\n",
    "def normalize_hand(\n",
    "    hand_3d_points: torch.Tensor | np.ndarray,\n",
    "    whrist_base: int = 0,\n",
    "    middle_finger_inner_bone: int = 8,\n",
    "    point_finger_inner_bone: int = 4,\n",
    "    eps: float = 1e-6,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize one hand (L,3) or a batch of hands (B, L, 3):\n",
    "      1. translate so point `whrist_base` is at the origin\n",
    "      2. rotate so `middle_finger_inner_bone` aligns with +Y\n",
    "      3. scale so that vector to `middle_finger_inner_bone` has length 1\n",
    "      4. rotate around Y so `point_finger_inner_bone` lies in the +Z half-plane\n",
    "    Returns same shape as input.\n",
    "    \"\"\"\n",
    "    if not isinstance(hand_3d_points, torch.Tensor):\n",
    "        hand_3d_points = torch.tensor(hand_3d_points, dtype=torch.float32)\n",
    "\n",
    "    # if batch of hands, just loop\n",
    "    if hand_3d_points.dim() == 3:\n",
    "        B, L, _ = hand_3d_points.shape\n",
    "        normalized = torch.empty_like(hand_3d_points)\n",
    "        for b in range(B):\n",
    "            # recursive call on each (L,3)\n",
    "            normalized[b] = normalize_hand(\n",
    "                hand_3d_points[b],\n",
    "                whrist_base,\n",
    "                middle_finger_inner_bone,\n",
    "                point_finger_inner_bone,\n",
    "                eps,\n",
    "            )\n",
    "        return normalized\n",
    "\n",
    "    # --- below is the original single‑hand logic for shape (L,3) ---\n",
    "    device, dtype = hand_3d_points.device, hand_3d_points.dtype\n",
    "\n",
    "    # 1) translate so wrist base → origin\n",
    "    T = -hand_3d_points[whrist_base]\n",
    "    hand = hand_3d_points + T\n",
    "\n",
    "    # 2) rotate so that middle finger inner bone aligns with +Y\n",
    "    target_y = torch.tensor([0, 1, 0], dtype=dtype, device=device)\n",
    "    R1 = rotation_matrix_from_vectors(hand[middle_finger_inner_bone], target_y, eps=eps)\n",
    "    hand = hand @ R1.T\n",
    "\n",
    "    # 3) scale so middle‐finger inner bone length → 1\n",
    "    hand = hand / (hand[middle_finger_inner_bone][1] + eps)\n",
    "\n",
    "    # 4) rotate around Y so the point finger inner bone lies in +Z half-plane\n",
    "    v = hand[point_finger_inner_bone]\n",
    "    xz = torch.stack([v[0], v[2]])\n",
    "    norm_xz = xz.norm()\n",
    "    if norm_xz >= eps:\n",
    "        sinA, cosA = xz[0] / norm_xz, xz[1] / norm_xz\n",
    "        R2 = torch.tensor(\n",
    "            [[cosA, 0.0, -sinA], [0.0, 1.0, 0.0], [sinA, 0.0, cosA]],\n",
    "            dtype=dtype,\n",
    "            device=device,\n",
    "        )\n",
    "        hand = hand @ R2.T\n",
    "\n",
    "    return hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c5ed500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Desired bone lengths\n",
    "BONE_LENGTHS = {\n",
    "    (0, 1): 7.0,\n",
    "    (1, 2): 3.5,\n",
    "    (2, 3): 2.5,  # Thumb\n",
    "    (0, 4): 9.0,\n",
    "    (4, 5): 4.0,\n",
    "    (5, 6): 2.5,\n",
    "    (6, 7): 2.0,  # Index\n",
    "    (0, 8): 9.0,\n",
    "    (8, 9): 5.0,\n",
    "    (9, 10): 3.0,\n",
    "    (10, 11): 2.0,  # Middle\n",
    "    (0, 12): 8.5,\n",
    "    (12, 13): 5.0,\n",
    "    (13, 14): 3.0,\n",
    "    (14, 15): 2.0,  # Ring\n",
    "    (0, 16): 8.0,\n",
    "    (16, 17): 4.0,\n",
    "    (17, 18): 2.5,\n",
    "    (18, 19): 2.0,  # Pinky\n",
    "}\n",
    "\n",
    "\n",
    "def fix_hand_landmarks_anatomy_batched(joints_batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize bone lengths of hand landmarks in a batched manner.\n",
    "    Returns normalized hand joints for each batch element.\n",
    "\n",
    "    Input:\n",
    "        joints_batch: 3D tensor of shape (B, 20, 3) of hand joint coordinates\n",
    "    Output:\n",
    "        3D tensor of shape (B, 20, 3) with normalized bone lengths\n",
    "    \"\"\"\n",
    "    fixed_batch = joints_batch.clone()\n",
    "    for i in range(joints_batch.shape[0]):\n",
    "        joints = joints_batch[i]\n",
    "        fixed = normalize_hand(joints)\n",
    "\n",
    "        for p, c in BONE_LENGTHS:\n",
    "            vec = joints[c] - joints[p]  # (3,)\n",
    "            length = torch.norm(vec)\n",
    "            if length > 0:\n",
    "                direction = vec / length\n",
    "            else:\n",
    "                direction = torch.zeros(3, device=joints.device)\n",
    "            target_length = BONE_LENGTHS[(p, c)]\n",
    "            fixed[c] = fixed[p] + direction * target_length\n",
    "\n",
    "        fixed_batch[i] = normalize_hand(fixed)\n",
    "\n",
    "    return fixed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aba38df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared import rot, _V, _P, DEFAULT_MORPHOLOGY\n",
    "import torch\n",
    "\n",
    "E = 1e-7\n",
    "\n",
    "\n",
    "def irot_full(\n",
    "    v: torch.Tensor,\n",
    "    p: torch.Tensor,\n",
    "    v_hat: torch.Tensor,\n",
    "    eps: torch.Tensor,  # Tolerance for \"cosA == 0\"\n",
    "    fallback_beta: torch.Tensor,\n",
    "):\n",
    "    \"\"\"\n",
    "    Batched inverse rotation function.\n",
    "\n",
    "    v, p, v_hat: (B, 3) tensors\n",
    "    Returns: alpha (B,), beta (B,), v_hat (B, 3), p_hat (B, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    q = torch.cross(v, p, dim=-1)\n",
    "\n",
    "    dot_v = v_hat.mul(v).sum(dim=-1)  # cosB*cosA\n",
    "    dot_p = v_hat.mul(p).sum(dim=-1)  # sinB*cosA\n",
    "    dot_q = v_hat.mul(q).sum(dim=-1).clamp(min=-1 + E, max=1 - E)  # sinA\n",
    "\n",
    "    alpha = torch.asin(dot_q)\n",
    "\n",
    "    cosA = torch.ones_like(dot_q).sub(dot_q.square()).clamp(min=0).sqrt()\n",
    "\n",
    "    beta = torch.where(\n",
    "        cosA.abs().lt(eps),\n",
    "        fallback_beta,\n",
    "        torch.atan2(dot_p.div(cosA), dot_v.div(cosA)),\n",
    "    )\n",
    "\n",
    "    p_hat = p.mul(beta.cos().unsqueeze(-1)).sub(v.mul(beta.sin().unsqueeze(-1)))\n",
    "\n",
    "    return alpha, beta, v_hat, p_hat\n",
    "\n",
    "\n",
    "def irot_alpha(\n",
    "    v: torch.Tensor,\n",
    "    p: torch.Tensor,\n",
    "    v_hat: torch.Tensor,\n",
    "):\n",
    "    \"\"\"\n",
    "    Batched inverse rotation function.\n",
    "\n",
    "    v, p, v_hat: (B, 3) tensors\n",
    "    Returns: alpha (B,), beta (B,), v_hat (B, 3), p_hat (B, 3)\n",
    "    \"\"\"\n",
    "    q = torch.cross(v, p, dim=-1)\n",
    "\n",
    "    cosA = v_hat.mul(v).sum(dim=-1).clamp(min=-1 + E, max=1 - E)\n",
    "    alpha = v_hat.mul(q).sum(dim=-1).sign().mul(torch.arccos(cosA))\n",
    "    beta = torch.zeros_like(cosA)\n",
    "    p_hat = p\n",
    "\n",
    "    return alpha, beta, v_hat, p_hat\n",
    "\n",
    "\n",
    "def inverse_hand_angles_by_landmarks(\n",
    "    landmarks: torch.Tensor,\n",
    "):\n",
    "    \"\"\"\n",
    "    landmarks: (B, 20, 3) - known hand landmarks in 3D space\n",
    "    morphology: (5, 10) - hand morphology (not batched)\n",
    "\n",
    "    Returns:\n",
    "    angles: (B, 20) - recovered joint angles\n",
    "    \"\"\"\n",
    "\n",
    "    B = landmarks.shape[0]\n",
    "\n",
    "    angles = torch.zeros(B, 5, 4, dtype=landmarks.dtype, device=landmarks.device)\n",
    "\n",
    "    morphology = torch.tensor(\n",
    "        DEFAULT_MORPHOLOGY,\n",
    "        dtype=angles.dtype,\n",
    "        device=angles.device,\n",
    "    )\n",
    "\n",
    "    # NOTE: assuming 0, 4, 8, 12, 16 landmarks are existing in the morphology\n",
    "    # 20 landmarks -> 21 landmarks, restoring thumb base from the morphology\n",
    "    #   so that now assuming 0, 1, 5, 9, 13, 17 are existing in the morphology\n",
    "    landmarks = torch.cat(\n",
    "        [\n",
    "            landmarks[:, :1, :],\n",
    "            morphology[0][3:6].expand(B, 1, 3),  # Repeat thumb_base across batch\n",
    "            landmarks[:, 1:, :],\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "    eps = torch.tensor(0.4, dtype=angles.dtype, device=angles.device)\n",
    "\n",
    "    V = torch.tensor(_V, dtype=angles.dtype, device=angles.device).expand(B, 3)\n",
    "    P = torch.tensor(_P, dtype=angles.dtype, device=angles.device).expand(B, 3)\n",
    "\n",
    "    for i, morph in enumerate(morphology):\n",
    "        base_idx = i * 4 + 1\n",
    "        alpha, beta, gamma, fallback_beta = morph[6:10].clone()\n",
    "\n",
    "        v, p = rot(V, P, alpha, beta)\n",
    "\n",
    "        # Rotate p around v according to parameter gamma\n",
    "        sinG, cosG = torch.sin(gamma), torch.cos(gamma)\n",
    "        p = p.mul(cosG).add(torch.cross(p, v, dim=-1).mul(sinG))\n",
    "\n",
    "        target = torch.nn.functional.normalize(\n",
    "            landmarks[:, base_idx + 1].sub(landmarks[:, base_idx]), dim=-1\n",
    "        )\n",
    "        alpha, beta, v, p = irot_full(v, p, target, eps, fallback_beta)\n",
    "        angles[:, i, 0] = alpha\n",
    "        angles[:, i, 1] = beta\n",
    "\n",
    "        for j in range(1, 3):\n",
    "            target = torch.nn.functional.normalize(\n",
    "                landmarks[:, base_idx + j + 1].sub(landmarks[:, base_idx + j]), dim=-1\n",
    "            )\n",
    "\n",
    "            alpha, beta, v, p = irot_alpha(v, p, target)\n",
    "\n",
    "            angles[:, i, j + 1] = alpha\n",
    "\n",
    "    return angles.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8270d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emg2pose.kinematics import forward_kinematics, load_default_hand_model\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "hand_model = load_default_hand_model()\n",
    "\n",
    "POINTS_SELECT = [5, 6, 7, 0, 8, 9, 10, 1, 11, 12, 13, 2, 14, 15, 16, 3, 17, 18, 19, 4]\n",
    "\n",
    "\n",
    "# x: B, C\n",
    "def emg2pose_forward_hand_kinematics(x: torch.Tensor):\n",
    "    x = x.unsqueeze(1).permute(0, 2, 1)\n",
    "    hands = forward_kinematics(x, hand_model).squeeze(1)\n",
    "    return hands[:, POINTS_SELECT, :]  # B, L, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8bb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "\n",
    "def archive_dataset(\n",
    "    grouped_segments: Dict[str, List[Tuple[int, int]]],\n",
    "    filepath: str,\n",
    "    desc: str,\n",
    "    filter_rec_len: int = 32 * 2,  # for 32 fps, thats 2 sec\n",
    "    filter_segment_len: int = 8,  # for 32 fps, thats 0.25 sec\n",
    "):\n",
    "    with DatasetWriter(filepath) as dataset_writer:\n",
    "        with tqdm(\n",
    "            total=sum(len(v) for v in grouped_segments.values()),\n",
    "            ncols=100,\n",
    "            desc=f\"Archiving {desc}\",\n",
    "        ) as pbar:\n",
    "            for path, segments in grouped_segments.items():\n",
    "                rec: HandEmgRecording = []\n",
    "                with h5py.File(path, \"r\") as f:\n",
    "                    timeseries = f[\"emg2pose\"][\"timeseries\"]  # type: ignore\n",
    "                    joint_angles = timeseries[\"joint_angles\"]  # type: ignore\n",
    "                    emg = timeseries[\"emg\"]  # type: ignore\n",
    "\n",
    "                    segment = None\n",
    "                    for slice in segments:\n",
    "                        start, end = slice[0], slice[1]\n",
    "                        slices = (end - start) // W\n",
    "                        real_end = slices * W + start\n",
    "\n",
    "                        segment = HandEmgRecordingSegment(couples=[], sigma=joint_angles[real_end])  # type: ignore\n",
    "\n",
    "                        for i in range(slices):  # type: ignore\n",
    "                            emg_slice = emg[start + i * W : start + (i + 1) * W]  # type: ignore\n",
    "                            joints = joint_angles[start + i * W]  # type: ignore\n",
    "                            segment.couples.append(HandEmgTuple(frame=joints, emg=emg_slice))  # type: ignore\n",
    "\n",
    "                        # Reencode frames to our format\n",
    "                        landmarks = emg2pose_forward_hand_kinematics(\n",
    "                            torch.tensor(segment.frames)\n",
    "                        )\n",
    "                        # new_frames = inverse_hand_angles_by_landmarks(\n",
    "                        #     fix_hand_landmarks_anatomy_batched(landmarks),\n",
    "                        # ).numpy()\n",
    "                        new_frames = inverse_hand_angles_by_landmarks(\n",
    "                            normalize_hand(landmarks),\n",
    "                        ).numpy()\n",
    "\n",
    "                        segment = HandEmgRecordingSegment(\n",
    "                            couples=[\n",
    "                                HandEmgTuple(\n",
    "                                    frame=new_frames[i],\n",
    "                                    emg=segment.couples[i].emg,\n",
    "                                )\n",
    "                                for i in range(slices)\n",
    "                            ],\n",
    "                            sigma=new_frames[-1],\n",
    "                        )\n",
    "\n",
    "                        if len(segment.couples) >= filter_segment_len:\n",
    "                            rec.append(segment)\n",
    "                        pbar.update(1)\n",
    "\n",
    "                if sum(len(segment.couples) for segment in rec) >= filter_rec_len:\n",
    "                    recording_writer = dataset_writer.add_recording()\n",
    "                    for segment in rec:\n",
    "                        recording_writer.add(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78a8765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segments(\n",
    "    file: str,\n",
    "    min_segment_length: int = 4096,\n",
    "    tail_trim: int = 512,\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract no ik failure segments from a emg2pose recording.\n",
    "    \"\"\"\n",
    "    # load raw segments\n",
    "    with h5py.File(file, \"r\") as f:\n",
    "        timeseries: np.ndarray = f[\"emg2pose\"][\"timeseries\"]  # type: ignore\n",
    "        joint_angles: np.ndarray = timeseries[\"joint_angles\"]  # (T, 20)  # type: ignore\n",
    "\n",
    "        # get ik_failure mask\n",
    "        zeros = np.zeros_like(joint_angles)\n",
    "        is_zero = np.isclose(joint_angles, zeros)\n",
    "        ik_failure_mask = ~np.all(is_zero, axis=-1)  # trues if no ik failure\n",
    "\n",
    "        ones = np.where(ik_failure_mask)[0]\n",
    "\n",
    "        if ones.shape[0] == 0:\n",
    "            # the whole file is ik failure\n",
    "            return []\n",
    "\n",
    "        boundaries = np.where(np.diff(ones) != 1)[0]\n",
    "        segments = [\n",
    "            (ones[i], ones[j])\n",
    "            for i, j in zip(\n",
    "                np.insert(boundaries + 1, 0, 0),\n",
    "                np.append(boundaries, len(ones) - 1),\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # trim tails of the segments since because of interpolation frames nearby the ik failure are not valid, so we need to throw them out\n",
    "    segments = [(s[0] + tail_trim, s[1] - tail_trim) for s in segments]\n",
    "    # there can be segments of negative length\n",
    "\n",
    "    # finally, filter segments by length\n",
    "    return [s for s in segments if (s[1] - s[0]) >= min_segment_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8270ddc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>user</th>\n",
       "      <th>stage</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>side</th>\n",
       "      <th>filename</th>\n",
       "      <th>moving_hand</th>\n",
       "      <th>held_out_user</th>\n",
       "      <th>held_out_stage</th>\n",
       "      <th>split</th>\n",
       "      <th>generalization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-07-1649318400-8125c-cv-emg-pose-train@2</td>\n",
       "      <td>29ddab35d7</td>\n",
       "      <td>ThumbsUpDownThumbRotationsCWCCWP</td>\n",
       "      <td>1.649400e+09</td>\n",
       "      <td>1.649400e+09</td>\n",
       "      <td>left</td>\n",
       "      <td>2022-04-07-1649318400-8125c-cv-emg-pose-train@...</td>\n",
       "      <td>both</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-07-1649318400-8125c-cv-emg-pose-train@2</td>\n",
       "      <td>29ddab35d7</td>\n",
       "      <td>ThumbsUpDownThumbRotationsCWCCWP</td>\n",
       "      <td>1.649400e+09</td>\n",
       "      <td>1.649400e+09</td>\n",
       "      <td>right</td>\n",
       "      <td>2022-04-07-1649318400-8125c-cv-emg-pose-train@...</td>\n",
       "      <td>both</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-07-1649318400-8125c-cv-emg-pose-train@2</td>\n",
       "      <td>29ddab35d7</td>\n",
       "      <td>HandClawGraspFlicks</td>\n",
       "      <td>1.649401e+09</td>\n",
       "      <td>1.649401e+09</td>\n",
       "      <td>left</td>\n",
       "      <td>2022-04-07-1649318400-8125c-cv-emg-pose-train@...</td>\n",
       "      <td>both</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-07-1649318400-8125c-cv-emg-pose-train@2</td>\n",
       "      <td>29ddab35d7</td>\n",
       "      <td>HandClawGraspFlicks</td>\n",
       "      <td>1.649401e+09</td>\n",
       "      <td>1.649401e+09</td>\n",
       "      <td>right</td>\n",
       "      <td>2022-04-07-1649318400-8125c-cv-emg-pose-train@...</td>\n",
       "      <td>both</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>val</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-07-1649318400-8125c-cv-emg-pose-train@2</td>\n",
       "      <td>29ddab35d7</td>\n",
       "      <td>ShakaVulcanPeace</td>\n",
       "      <td>1.649401e+09</td>\n",
       "      <td>1.649401e+09</td>\n",
       "      <td>left</td>\n",
       "      <td>2022-04-07-1649318400-8125c-cv-emg-pose-train@...</td>\n",
       "      <td>both</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>val</td>\n",
       "      <td>user_stage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           session        user  \\\n",
       "0  2022-04-07-1649318400-8125c-cv-emg-pose-train@2  29ddab35d7   \n",
       "1  2022-04-07-1649318400-8125c-cv-emg-pose-train@2  29ddab35d7   \n",
       "2  2022-04-07-1649318400-8125c-cv-emg-pose-train@2  29ddab35d7   \n",
       "3  2022-04-07-1649318400-8125c-cv-emg-pose-train@2  29ddab35d7   \n",
       "4  2022-04-07-1649318400-8125c-cv-emg-pose-train@2  29ddab35d7   \n",
       "\n",
       "                              stage         start           end   side  \\\n",
       "0  ThumbsUpDownThumbRotationsCWCCWP  1.649400e+09  1.649400e+09   left   \n",
       "1  ThumbsUpDownThumbRotationsCWCCWP  1.649400e+09  1.649400e+09  right   \n",
       "2               HandClawGraspFlicks  1.649401e+09  1.649401e+09   left   \n",
       "3               HandClawGraspFlicks  1.649401e+09  1.649401e+09  right   \n",
       "4                  ShakaVulcanPeace  1.649401e+09  1.649401e+09   left   \n",
       "\n",
       "                                            filename moving_hand  \\\n",
       "0  2022-04-07-1649318400-8125c-cv-emg-pose-train@...        both   \n",
       "1  2022-04-07-1649318400-8125c-cv-emg-pose-train@...        both   \n",
       "2  2022-04-07-1649318400-8125c-cv-emg-pose-train@...        both   \n",
       "3  2022-04-07-1649318400-8125c-cv-emg-pose-train@...        both   \n",
       "4  2022-04-07-1649318400-8125c-cv-emg-pose-train@...        both   \n",
       "\n",
       "   held_out_user  held_out_stage split generalization  \n",
       "0           True           False   val           user  \n",
       "1           True           False   val           user  \n",
       "2           True           False   val           user  \n",
       "3           True           False   val           user  \n",
       "4           True            True   val     user_stage  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base_path = \"C:/Users/shich/emg2pose_data\"\n",
    "\n",
    "metadata = pd.read_csv(f\"{base_path}/metadata.csv\")\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be4d8384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that no users has collisions by slice of the id\n",
    "users = metadata[\"user\"].unique()\n",
    "first_two_symbols = {user[-4:] for user in users}\n",
    "has_collisions = len(first_two_symbols) != len(users)\n",
    "has_collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ae5014a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 751 sessions\n"
     ]
    }
   ],
   "source": [
    "sessions = metadata[\"session\"].unique()\n",
    "print(f\"Found {len(sessions)} sessions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3fc88b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Archiving 1/8: 100%|████████████████████████████████████████████████| 75/75 [00:42<00:00,  1.75it/s]\n",
      "Archiving 2/8: 100%|████████████████████████████████████████████████| 32/32 [00:14<00:00,  2.17it/s]\n",
      "Archiving 3/8: 100%|████████████████████████████████████████████████| 63/63 [00:29<00:00,  2.10it/s]\n",
      "Archiving 4/8: 100%|████████████████████████████████████████████████| 74/74 [00:43<00:00,  1.70it/s]\n",
      "Archiving 5/8: 100%|████████████████████████████████████████████████| 82/82 [00:29<00:00,  2.74it/s]\n",
      "Archiving 6/8: 100%|████████████████████████████████████████████████| 62/62 [00:50<00:00,  1.23it/s]\n",
      "Archiving 7/8: 100%|████████████████████████████████████████████████| 83/83 [00:28<00:00,  2.87it/s]\n",
      "Archiving 8/8: 100%|████████████████████████████████████████████████| 63/63 [00:43<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "count = 8\n",
    "sessions = sessions[start : start + count]\n",
    "\n",
    "# for each session, load recordings, split to segments and archive them as a separate dataset\n",
    "for i, session in enumerate(sessions):\n",
    "    user = metadata[metadata[\"session\"] == session][\"user\"].unique()[0]\n",
    "\n",
    "    # load left hand recordings for the session\n",
    "    recordings = metadata[\n",
    "        (metadata[\"session\"] == session) & (metadata[\"side\"] == \"left\")\n",
    "    ][\"filename\"].unique()\n",
    "\n",
    "    # load segments from each recording\n",
    "    grouped_segments = {}\n",
    "    for recording in recordings:\n",
    "        fname = f\"{base_path}/{recording}.hdf5\"\n",
    "        segments = extract_segments(fname)\n",
    "\n",
    "        if len(segments) == 0:\n",
    "            continue\n",
    "\n",
    "        grouped_segments[fname] = segments\n",
    "\n",
    "    if len(grouped_segments.keys()) == 0:\n",
    "        raise ValueError(f\"Cannot proper segmentate recordings from {session}\")\n",
    "\n",
    "    # archive segments to a dataset\n",
    "    archive_dataset(\n",
    "        grouped_segments,\n",
    "        f\"../datasets/s{start + i + 1}.z\",\n",
    "        desc=f\"{i + 1}/{len(sessions)}\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
